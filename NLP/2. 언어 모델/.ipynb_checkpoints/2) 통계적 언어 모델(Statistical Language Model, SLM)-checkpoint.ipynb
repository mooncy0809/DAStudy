{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa6dedb",
   "metadata": {},
   "source": [
    "## 통계적 언어 모델\n",
    "언어모델의 전통적인 접근 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75c9d1",
   "metadata": {},
   "source": [
    "1. 조건부 확률\n",
    "2. 문장에 대한 확률\n",
    "3. 카운트 기반의 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1400a",
   "metadata": {},
   "source": [
    "4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
    "언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링 합니다. 실제로 정확하게 알아볼 방법은 없겠지만 현실에서도 An adorable little boy가 나왔을 때 is가 나올 확률이라는 것이 존재합니다. 이를 실제 자연어의 확률 분포, 현실에서의 확률 분포라고 명칭합시다. 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표입니다. 그런데 카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스(corpus). 즉, 다시 말해 기계가 훈련하는 데이터는 정말 방대한 양이 필요합니다.\n",
    "이때 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 희소 문제(sparsity problem)라고 합니다.\n",
    "\n",
    "위 문제를 완화하는 방법으로 바로 이어서 배우게 되는 n-gram 언어 모델이나 스무딩이나 백오프와 같은 여러가지 일반화(generalization)기법이 존재합니다. 하지만 희소 문제에 대한 근본적인 해결책은 되지 못하였습니다. 결국 이러한 한계로 인해 언어 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어가게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329fc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naver_crawler",
   "language": "python",
   "name": "naver_crawler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
